{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Generation-using-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOIepmKJIuOJ",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFk2jeEvzVSZ",
        "colab_type": "text"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da5Xqd5TzJQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_GH_sDwX5uT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaab50ec-9d5b-43f1-e55c-159a4a00559d"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOHcf2B34DM",
        "colab_type": "text"
      },
      "source": [
        "#Here i am downloading the dataset for text generation.\n",
        "##I have downloaded the article \"Leo Tolstoy's War and Peace\" from adnrej-karpathy blog.\n",
        "##I have used get method of requests library. Then i recieved the content in \"requests.models.Response\" form then i am converting it into str."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcgiTlYUt0CT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "87bd699e-69d6-4db8-8818-bc524827a6d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqghll3hzUtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = requests.get(\"http://www.gutenberg.org/cache/epub/11/pg11.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PTi_3zaGdPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.utils.data_utils import get_file\n",
        "# import\n",
        "# path = get_file(\n",
        "#     'nietzsche.txt',\n",
        "#     origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "# with io.open(path, encoding='utf-8') as f:\n",
        "#     text = f.read().lower()\n",
        "# print('corpus length:', len(text))\n",
        "# type(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51mt5dyI55NY",
        "colab_type": "text"
      },
      "source": [
        "##Type of content i recieved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91hEE2IS5gro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbc2b788-b49f-454b-d891-f3327669ba64"
      },
      "source": [
        "type(r)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "requests.models.Response"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Ui92Xi5_u_",
        "colab_type": "text"
      },
      "source": [
        "##Converting the text recived in str form so that we can operate on it.\n",
        "##I have printed first 500 text of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_T8yR2f5UDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d595ae98-0802-4f5b-a738-c54ed5628edf"
      },
      "source": [
        "raw_txt = r.text\n",
        "print(\"The type is : {}\".format(type(raw_txt)))\n",
        "print(raw_txt[800:1500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The type is : <class 'str'>\n",
            " sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, 'and what is the use of a book,' thought Alice 'without pictures or\r\n",
            "conversations?'\r\n",
            "\r\n",
            "So she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a daisy-chain would be worth the trouble of getting up and\r\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\r\n",
            "close by her.\r\n",
            "\r\n",
            "There was nothing so VERY remarkable in that; nor did Alice think it so\r\n",
            "VERY much out of the way to hear the Rabbit say to itself, '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmVJ_2F76h8w",
        "colab_type": "text"
      },
      "source": [
        "##Now i am converting the all text in lower format and using regulare expressions library for exclusion of non-asciii text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttDV5ntnzkYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_text = raw_txt.lower()\n",
        "p_text = re.sub(r'[^\\x00-\\x7f]',r'', p_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpXETEjU6yxE",
        "colab_type": "text"
      },
      "source": [
        "##The type of text is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU9E4ZzL62V9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b1401ab4-cff4-41a3-fe29-d82e03f198ea"
      },
      "source": [
        "print(\"the type of processed text is : \" ,type(p_text))\n",
        "type(p_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the type of processed text is :  <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_vB0JIH7HA3",
        "colab_type": "text"
      },
      "source": [
        "##Printing1000 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6F5PiDJ4Rmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "eaf4ff5e-21f4-4908-8a38-07dff552090d"
      },
      "source": [
        "print(p_text[900:1900])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, 'and what is the use of a book,' thought alice 'without pictures or\r\n",
            "conversations?'\r\n",
            "\r\n",
            "so she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a daisy-chain would be worth the trouble of getting up and\r\n",
            "picking the daisies, when suddenly a white rabbit with pink eyes ran\r\n",
            "close by her.\r\n",
            "\r\n",
            "there was nothing so very remarkable in that; nor did alice think it so\r\n",
            "very much out of the way to hear the rabbit say to itself, 'oh dear!\r\n",
            "oh dear! i shall be late!' (when she thought it over afterwards, it\r\n",
            "occurred to her that she ought to have wondered at this, but at the time\r\n",
            "it all seemed quite natural); but when the rabbit actually took a watch\r\n",
            "out of its waistcoat-pocket, and looked at it, and then hurried on,\r\n",
            "alice started to her feet, for it flashed across her mind that she had\r\n",
            "never before seen a rabbit with ei\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embXW2fK7mgB",
        "colab_type": "text"
      },
      "source": [
        "#Creating a list of chars which is a set of unique chars(also sorted in ascending manner) used in the whole dataset\n",
        "##Printing out the length of chars in whole dataset\n",
        "##Printing the length of unique chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3fQCRMR4Y5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "641c906e-d6fb-4d81-bb76-00ff1bcdb210"
      },
      "source": [
        "chars = sorted(list(set(p_text)))\n",
        "print(\"Total length of file  : {}\".format(len(p_text)))\n",
        "print(\"Total length of chars : {}\".format(len(chars)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length of file  : 167515\n",
            "Total length of chars : 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESu7YlDj7-vz",
        "colab_type": "text"
      },
      "source": [
        "##Printing out the unique set of chars in a sorted manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91t9v09L74o5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eaf6df52-5435-4447-814e-3d3ea13769f4"
      },
      "source": [
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8DesI7H8OTs",
        "colab_type": "text"
      },
      "source": [
        "##A neural network works with numbers, not text characters. So well  convert the characters in our input to numbers. We'll sort the list of the set of all characters that appear in our input text, then use the enumerate function to get numbers which represent the characters. We then create a dictionary that stores the keys and values, or the characters and the numbers that represent them.\n",
        "\n",
        "##Similarly we will create a dictinary to map those numbber back to character so that when we use our model for prediction, it gives text as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaHL8-FK8rvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
        "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JAovpJ381Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe167ad6-5f5f-49ef-8591-996b669877c2"
      },
      "source": [
        "ix_to_char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: '\\r',\n",
              " 2: ' ',\n",
              " 3: '!',\n",
              " 4: '\"',\n",
              " 5: '#',\n",
              " 6: '$',\n",
              " 7: '%',\n",
              " 8: \"'\",\n",
              " 9: '(',\n",
              " 10: ')',\n",
              " 11: '*',\n",
              " 12: ',',\n",
              " 13: '-',\n",
              " 14: '.',\n",
              " 15: '/',\n",
              " 16: '0',\n",
              " 17: '1',\n",
              " 18: '2',\n",
              " 19: '3',\n",
              " 20: '4',\n",
              " 21: '5',\n",
              " 22: '6',\n",
              " 23: '7',\n",
              " 24: '8',\n",
              " 25: '9',\n",
              " 26: ':',\n",
              " 27: ';',\n",
              " 28: '?',\n",
              " 29: '@',\n",
              " 30: '[',\n",
              " 31: ']',\n",
              " 32: '_',\n",
              " 33: 'a',\n",
              " 34: 'b',\n",
              " 35: 'c',\n",
              " 36: 'd',\n",
              " 37: 'e',\n",
              " 38: 'f',\n",
              " 39: 'g',\n",
              " 40: 'h',\n",
              " 41: 'i',\n",
              " 42: 'j',\n",
              " 43: 'k',\n",
              " 44: 'l',\n",
              " 45: 'm',\n",
              " 46: 'n',\n",
              " 47: 'o',\n",
              " 48: 'p',\n",
              " 49: 'q',\n",
              " 50: 'r',\n",
              " 51: 's',\n",
              " 52: 't',\n",
              " 53: 'u',\n",
              " 54: 'v',\n",
              " 55: 'w',\n",
              " 56: 'x',\n",
              " 57: 'y',\n",
              " 58: 'z'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EIO7Led86sD",
        "colab_type": "text"
      },
      "source": [
        "##So we will create a model with character level generation\n",
        "##We will create a subsequence of 80 chars which will be given as input to the model and the model will predict the 81st character.\n",
        "##Similarly with the concept of LSTM, RNN, the model will also remember the value of previous output which will help in text generation or creation of a line.LSTM will easily remeber the previous chars in a sentence over a long period.\n",
        "\n",
        "##So we have created input sequence and output sequence\n",
        "##Similarly we have created x_training and y_training data which consists of integer mapped from the dictionary we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ3XmK7k9vi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a8d103c7-ca7c-4151-e739-0766090c6a8e"
      },
      "source": [
        "maxlen = 80\n",
        "x_data = []\n",
        "y_data = []\n",
        "for i in range(0, len(p_text) - maxlen, 1):\n",
        "    in_seq  = p_text[i: i + maxlen]\n",
        "    out_seq = p_text[i + maxlen]\n",
        "    x_data.append([char_to_ix[char] for char in in_seq])\n",
        "    y_data.append([char_to_ix[out_seq]])\n",
        "nb_chars = len(x_data)\n",
        "print('total chars : {}'.format(nb_chars))\n",
        "print('nb sentences:', int(len(x_data)/80))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars : 167435\n",
            "nb sentences: 2092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8YQa8oS_Ipr",
        "colab_type": "text"
      },
      "source": [
        "##So we have 40726 lines each consisting of 80 chars.\n",
        "##Here i am printing the first 5 words of 1st line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcT0OcJiuoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "100d8372-dbeb-4d09-a13b-33171309a823"
      },
      "source": [
        "print(type(x_data))\n",
        "x_data[1][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50, 47, 42, 37, 35]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsZD-E4p_Tt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca8021c4-997a-40f8-b8f1-13586a806fd4"
      },
      "source": [
        "in_seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ebooks, and how to\\r\\nsubscribe to our email newsletter to hear about new ebooks.\\r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiDx1Qjq_hZV",
        "colab_type": "text"
      },
      "source": [
        "##Printing the first 5 target values of y_training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtCaY58hIUdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d401c4eb-f008-4a2b-ca7c-6ca82ca95fd8"
      },
      "source": [
        "y_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34], [47], [47], [43], [2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPzcd0dn_oTI",
        "colab_type": "text"
      },
      "source": [
        "##Now we'll go ahead and convert our input sequences into a processed numpy array that our network can use. We'll also need to convert the numpy array values into floats so that the sigmoid activation function our network uses can interpret them and output probabilities from 0 to 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WihHRtVOuZ-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.reshape(x_data , (nb_chars , maxlen , 1))\n",
        "n_vocab = len(chars)\n",
        "x = x/float(n_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xXlm6aMCvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7722faab-1c6b-4cc9-ce6f-63f166483aa6"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167435, 80, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLYc5lI4wlj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4e991cca-749a-4ea1-c0a8-7a57acce61ae"
      },
      "source": [
        "print(type(x))\n",
        "x[1][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.84745763],\n",
              "       [0.79661017],\n",
              "       [0.71186441],\n",
              "       [0.62711864],\n",
              "       [0.59322034]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd_ivOfaAq0b",
        "colab_type": "text"
      },
      "source": [
        "##Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 53 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijNzDlhjwvvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = tf.keras.utils.to_categorical(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewuVFj2zAvzW",
        "colab_type": "text"
      },
      "source": [
        "##printing out the representation of first 2 target variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCHivdBsw6Yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "159fbc60-5200-47f6-d161-9899e49559aa"
      },
      "source": [
        "print(type(y))\n",
        "y[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspIW59vZugi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5b37a7d-3b91-4c28-f4c0-e2bb4a6791e2"
      },
      "source": [
        "print(\"The shape of x_training data : \" ,x.shape)\n",
        "print(\"The shape of y_training data : \" ,y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of x_training data :  (167435, 80, 1)\n",
            "The shape of y_training data :  (167435, 59)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqGF6-nOBEiP",
        "colab_type": "text"
      },
      "source": [
        "##Creating our model with 4 LSTM layers and 1 Dense layer\n",
        "###The first LSTM layer has a input shape of (80 ,1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnDen6yyMHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.LSTM(256 , input_shape = (80 , 1) , return_sequences=True),\n",
        "                             tf.keras.layers.Dropout(0.3),\n",
        "                             tf.keras.layers.LSTM(256, return_sequences=True),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.LSTM(64),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.Dense(59, activation= 'softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BnR8wrp0LVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "cb99fa07-b285-4562-eb28-b03b7c370406"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 80, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 80, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 80, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 80, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                82176     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 59)                3835      \n",
            "=================================================================\n",
            "Total params: 875,515\n",
            "Trainable params: 875,515\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZ8OlLYBXtW",
        "colab_type": "text"
      },
      "source": [
        "##Compiling our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ruFD520PHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AydrQFD6Bom-",
        "colab_type": "text"
      },
      "source": [
        "##The network is slow to train, it takes 15 minutes to train one epoch.\n",
        "##Model checkpoint is used to save the network weights at the end of the epoch \n",
        "##We will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch.\n",
        "##We'll save the weights and reload them when the training is finished. We'll set a checkpoint to save the weights, and then make them the callbacks for our future model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyhwA9UV1MB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model_callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJIzc0-J09Eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5cd67f5-7a03-4f86-fe9a-2537c5251214"
      },
      "source": [
        "Model.fit(x, y , batch_size = 50, epochs = 60 , callbacks = model_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.9545\n",
            "Epoch 00001: loss improved from inf to 2.95446, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.9545\n",
            "Epoch 2/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.6649\n",
            "Epoch 00002: loss improved from 2.95446 to 2.66489, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.6649\n",
            "Epoch 3/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.5341\n",
            "Epoch 00003: loss improved from 2.66489 to 2.53405, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.5341\n",
            "Epoch 4/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.3975\n",
            "Epoch 00004: loss improved from 2.53405 to 2.39754, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.3975\n",
            "Epoch 5/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.2991\n",
            "Epoch 00005: loss improved from 2.39754 to 2.29907, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.2991\n",
            "Epoch 6/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.2245\n",
            "Epoch 00006: loss improved from 2.29907 to 2.22449, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.2245\n",
            "Epoch 7/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.1563\n",
            "Epoch 00007: loss improved from 2.22449 to 2.15626, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.1563\n",
            "Epoch 8/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.1061\n",
            "Epoch 00008: loss improved from 2.15626 to 2.10606, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.1061\n",
            "Epoch 9/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.0589\n",
            "Epoch 00009: loss improved from 2.10606 to 2.05886, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.0589\n",
            "Epoch 10/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 2.0174\n",
            "Epoch 00010: loss improved from 2.05886 to 2.01741, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 2.0174\n",
            "Epoch 11/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.9813\n",
            "Epoch 00011: loss improved from 2.01741 to 1.98133, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.9813\n",
            "Epoch 12/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.9445\n",
            "Epoch 00012: loss improved from 1.98133 to 1.94454, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.9445\n",
            "Epoch 13/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.9199\n",
            "Epoch 00013: loss improved from 1.94454 to 1.91993, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.9199\n",
            "Epoch 14/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.8945\n",
            "Epoch 00014: loss improved from 1.91993 to 1.89448, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.8945\n",
            "Epoch 15/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.8647\n",
            "Epoch 00015: loss improved from 1.89448 to 1.86470, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.8647\n",
            "Epoch 16/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.8485\n",
            "Epoch 00016: loss improved from 1.86470 to 1.84852, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.8485\n",
            "Epoch 17/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.8236\n",
            "Epoch 00017: loss improved from 1.84852 to 1.82358, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.8236\n",
            "Epoch 18/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.8081\n",
            "Epoch 00018: loss improved from 1.82358 to 1.80810, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.8081\n",
            "Epoch 19/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7900\n",
            "Epoch 00019: loss improved from 1.80810 to 1.79002, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7900\n",
            "Epoch 20/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7730\n",
            "Epoch 00020: loss improved from 1.79002 to 1.77298, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7730\n",
            "Epoch 21/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7583\n",
            "Epoch 00021: loss improved from 1.77298 to 1.75829, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7583\n",
            "Epoch 22/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7439\n",
            "Epoch 00022: loss improved from 1.75829 to 1.74387, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7439\n",
            "Epoch 23/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7309\n",
            "Epoch 00023: loss improved from 1.74387 to 1.73087, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7309\n",
            "Epoch 24/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7159\n",
            "Epoch 00024: loss improved from 1.73087 to 1.71590, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7159\n",
            "Epoch 25/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.7056\n",
            "Epoch 00025: loss improved from 1.71590 to 1.70561, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.7056\n",
            "Epoch 26/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6931\n",
            "Epoch 00026: loss improved from 1.70561 to 1.69310, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.6931\n",
            "Epoch 27/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6860\n",
            "Epoch 00027: loss improved from 1.69310 to 1.68603, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6860\n",
            "Epoch 28/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6728\n",
            "Epoch 00028: loss improved from 1.68603 to 1.67279, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6728\n",
            "Epoch 29/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6679\n",
            "Epoch 00029: loss improved from 1.67279 to 1.66787, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6679\n",
            "Epoch 30/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6563\n",
            "Epoch 00030: loss improved from 1.66787 to 1.65631, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6563\n",
            "Epoch 31/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6452\n",
            "Epoch 00031: loss improved from 1.65631 to 1.64525, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6452\n",
            "Epoch 32/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6372\n",
            "Epoch 00032: loss improved from 1.64525 to 1.63718, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6372\n",
            "Epoch 33/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6293\n",
            "Epoch 00033: loss improved from 1.63718 to 1.62931, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6293\n",
            "Epoch 34/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6195\n",
            "Epoch 00034: loss improved from 1.62931 to 1.61945, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6195\n",
            "Epoch 35/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.6140\n",
            "Epoch 00035: loss improved from 1.61945 to 1.61396, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6140\n",
            "Epoch 36/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.6093\n",
            "Epoch 00036: loss improved from 1.61396 to 1.60925, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.6093\n",
            "Epoch 37/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.5996\n",
            "Epoch 00037: loss improved from 1.60925 to 1.59944, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5994\n",
            "Epoch 38/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5907\n",
            "Epoch 00038: loss improved from 1.59944 to 1.59074, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5907\n",
            "Epoch 39/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5873\n",
            "Epoch 00039: loss improved from 1.59074 to 1.58726, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5873\n",
            "Epoch 40/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5817\n",
            "Epoch 00040: loss improved from 1.58726 to 1.58172, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5817\n",
            "Epoch 41/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5751\n",
            "Epoch 00041: loss improved from 1.58172 to 1.57512, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5751\n",
            "Epoch 42/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5721\n",
            "Epoch 00042: loss improved from 1.57512 to 1.57214, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5721\n",
            "Epoch 43/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5628\n",
            "Epoch 00043: loss improved from 1.57214 to 1.56276, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5628\n",
            "Epoch 44/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5597\n",
            "Epoch 00044: loss improved from 1.56276 to 1.55965, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5597\n",
            "Epoch 45/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5538\n",
            "Epoch 00045: loss improved from 1.55965 to 1.55375, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5538\n",
            "Epoch 46/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.5504\n",
            "Epoch 00046: loss improved from 1.55375 to 1.55046, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5505\n",
            "Epoch 47/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5461\n",
            "Epoch 00047: loss improved from 1.55046 to 1.54606, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.5461\n",
            "Epoch 48/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5379\n",
            "Epoch 00048: loss improved from 1.54606 to 1.53794, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5379\n",
            "Epoch 49/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.5387\n",
            "Epoch 00049: loss did not improve from 1.53794\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5386\n",
            "Epoch 50/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5315\n",
            "Epoch 00050: loss improved from 1.53794 to 1.53147, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5315\n",
            "Epoch 51/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.5268\n",
            "Epoch 00051: loss improved from 1.53147 to 1.52682, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 67s 20ms/step - loss: 1.5268\n",
            "Epoch 52/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5239\n",
            "Epoch 00052: loss improved from 1.52682 to 1.52388, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.5239\n",
            "Epoch 53/60\n",
            "3348/3349 [============================>.] - ETA: 0s - loss: 1.5191\n",
            "Epoch 00053: loss improved from 1.52388 to 1.51911, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.5191\n",
            "Epoch 54/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5166\n",
            "Epoch 00054: loss improved from 1.51911 to 1.51665, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.5166\n",
            "Epoch 55/60\n",
            "3349/3349 [==============================] - ETA: 0s - loss: 1.5145\n",
            "Epoch 00055: loss improved from 1.51665 to 1.51453, saving model to model_weights_saved.hdf5\n",
            "3349/3349 [==============================] - 66s 20ms/step - loss: 1.5145\n",
            "Epoch 56/60\n",
            "2227/3349 [==================>...........] - ETA: 22s - loss: 1.5068Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akBd9xKyCztF",
        "colab_type": "text"
      },
      "source": [
        "##Now we will load our model and recompileit in the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0daQeywpcz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73ace316-6f89-468c-e9a3-fbb86dd25347"
      },
      "source": [
        "cd '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO-50D5DvjrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model.save('my_rnn_new_model.h5')\n",
        "Model.save_weights('model_weights_saved.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPr7KSinwEzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model.save_weights('my_rnn__new_model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9LC_Kf_1jRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "Model.load_weights(filename)\n",
        "Model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dyLpdhVDEz8",
        "colab_type": "text"
      },
      "source": [
        "##Now testing our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snx3a_ZXDND4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = np.random.randint(0, len(x_data) - 2)\n",
        "pattern = x_data[start]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_jkztnQC1R3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5be2669-f7bd-4f7a-d315-8ea15dd97c8e"
      },
      "source": [
        "len(pattern)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvLL316gqJ9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = []\n",
        "seed = 'there was nothing so very remarkable in that nor did alice think it so very much'\n",
        "for i in seed:\n",
        "    value = char_to_ix[i]\n",
        "    pattern.append(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6H2k3WwFJ_-",
        "colab_type": "text"
      },
      "source": [
        "##Text generation from the model we trained on given training data and on 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3QxTdPtJgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0afddc11-fdcd-4835-ab99-444ffa32ec84"
      },
      "source": [
        "print(seed)\n",
        "for i in range(100):\n",
        "  X = np.reshape(pattern , (1, len(pattern) , 1))\n",
        "  X = X/float(n_vocab)\n",
        "  int_prediction = Model.predict(X , verbose = 0)\n",
        "  index = np.argmax(int_prediction)\n",
        "  prediction = ix_to_char[index]\n",
        "  seq_in = [ix_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(prediction)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there was nothing so very remarkable in that nor did alice think it so very much\n",
            " on the \n",
            "coor of the searers had aeains and she was a little shought it was \n",
            "the hatter was a little"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpXAJbrcs_KY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "cd858d69-f32f-4235-e01f-f5da04284817"
      },
      "source": [
        "print(seed)\n",
        "for i in range(1000):\n",
        "  X = np.reshape(pattern , (1, len(pattern) , 1))\n",
        "  X = X/float(n_vocab)\n",
        "  int_prediction = Model.predict(X , verbose = 0)\n",
        "  index = np.argmax(int_prediction)\n",
        "  prediction = ix_to_char[index]\n",
        "  seq_in = [ix_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(prediction)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there was nothing so very remarkable in that nor did alice think it so very much\n",
            " shought the way of the words were tead of the\n",
            "sead of the sooe of the searers took of the sea the white rabbit \n",
            "and the white rabbit shall she sat of the mouse say of the coor of the\n",
            "sealers of the sea and she was a little shall had foot to the thought\n",
            "the white rabbit and the white rabbit shat she was a little shought\n",
            "all the way of the white rabbit shat she was a little shought ali \n",
            "\n",
            "sealer of the sea the white rabbit shat she was a little shought the\n",
            "sears a little shought the way of the coor of the sooe of the sealen\n",
            "to to the sea the white rabbit sead of the sooe of the sooe of the\n",
            "sea-'\n",
            "\n",
            "'i don't know it to the word it would not she say the sea!' said the\n",
            "goython,\n",
            "\n",
            "'i don't know it to the word it would not she sea,' said the mock turtle\n",
            "\n",
            "sea- and she was a little shought the mooent alice had goot the satter\n",
            "of the sooe of the sooe "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAii3O27Jq5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIbaNGUTJv_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}